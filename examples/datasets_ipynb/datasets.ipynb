{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "\n",
    "CURRENT_DIR = '/code/examples/datasets_ipynb'\n",
    "\n",
    "logging.config.fileConfig(\n",
    "    os.path.join(CURRENT_DIR, 'logging.conf'),\n",
    "    disable_existing_loggers=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file defines classes that hides the logic/path for saving and loading specific datasets that\n",
    "are used across this project, as well as providing a brief description for each dataset.\n",
    "\n",
    "To define a new dataset, create a property in Datasets.__init__() following the existing patthern.\n",
    "\n",
    "The DATA variable is assigned an instance of the Datasets class and can be imported into other\n",
    "scripts/notebooks.\n",
    "\n",
    "To load the dataset called `the_dataset`, use the following code:\n",
    "\n",
    "```\n",
    "from source.services.data import DATA\n",
    "df = DATA.the_dataset.load()\n",
    "```\n",
    "\n",
    "To save the dataset called `the_dataset`, use the following code:\n",
    "\n",
    "```\n",
    "from source.services.data import DATA\n",
    "\n",
    "df = ...logic..\n",
    "DATA.the_dataset.save(df)\n",
    "```\n",
    "\"\"\"\n",
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "\n",
    "def read_pickle(path):\n",
    "    \"\"\"\n",
    "    Simple helper function that read's from a pickled object.\n",
    "\n",
    "    Args:\n",
    "        path:\n",
    "            File path where the pickled object will be stored.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path, 'rb') as handle:\n",
    "        unpickled_object = pickle.load(handle)\n",
    "    return unpickled_object\n",
    "\n",
    "\n",
    "def to_pickle(obj, path):\n",
    "    \"\"\"\n",
    "    Helper function that saves a pickled object.\n",
    "\n",
    "    NOTE: If there is an existing file that matches `path`, it will rename that file by appending\n",
    "    the current timestamp. The intent is to cache files in case there is an issue or any desire\n",
    "    to look at past datasets.\n",
    "\n",
    "    Args:\n",
    "        obj:\n",
    "            the object to save\n",
    "        path:\n",
    "            File path where the pickled object will be read from.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(path):\n",
    "        timestamp = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "        os.rename(path, path + '.' + timestamp)\n",
    "\n",
    "    with open(path, 'wb') as handle:\n",
    "        pickle.dump(obj, handle)\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"class that wraps the logic of saving/loading/describing a given dataset.\"\"\"\n",
    "    def __init__(self, name: str, description: str, dependencies: list, path: str) -> None:\n",
    "        self._name = name\n",
    "        self._description = description\n",
    "        self._dependencies = dependencies\n",
    "        self._path = path\n",
    "\n",
    "    def load(self):\n",
    "        logging.info(f\"Loading data `{self._name}` from `{self._path}`\")\n",
    "        return read_pickle(path=self._path)\n",
    "\n",
    "    def save(self, data):\n",
    "        logging.info(f\"Saving data `{self._name}` to `{self._path}`\")\n",
    "        to_pickle(obj=data, path=self._path)\n",
    "\n",
    "\n",
    "class Datasets:\n",
    "    \"\"\"class that defines all of the datasets available globally to the project.\"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Use this function to define datasets by following the existing pattern.\"\"\"\n",
    "        self.dataset_1 = DataLoader(\n",
    "            name='dataset_1',\n",
    "            description=\"Dataset description\",\n",
    "            dependencies=['SNOWFLAKE.SCHEMA.TABLE'],\n",
    "            path='data/dataset_1.pkl',\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def datasets(self) -> list[str]:\n",
    "        \"\"\"Returns the names of the datasets available.\"\"\"\n",
    "        ignore = set(['datasets', 'descriptions', 'dependencies'])\n",
    "        return [\n",
    "            attr for attr in dir(self)\n",
    "            if attr not in ignore and isinstance(getattr(self, attr), DataLoader)\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def descriptions(self) -> dict[str]:\n",
    "        \"\"\"Returns the names and descriptions of the datasets available.\"\"\"\n",
    "        return [\n",
    "            dict(\n",
    "                dataset=getattr(self, x)._name,\n",
    "                description=getattr(self, x)._description\n",
    "            )\n",
    "            for x in self.datasets\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def dependencies(self) -> dict[str]:\n",
    "        \"\"\"Returns the names and dependencies of the datasets available.\"\"\"\n",
    "        return [\n",
    "            dict(\n",
    "                dataset=getattr(self, x)._name,\n",
    "                dependencies=getattr(self, x)._dependencies\n",
    "            )\n",
    "            for x in self.datasets\n",
    "        ]\n",
    "\n",
    "\n",
    "# create a global object that can be imported into other scripts\n",
    "DATA = Datasets()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset_1.pkl', 'dataset_1.pkl.2023_03_04_19_45_16']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('/code/examples/datasets_ipynb/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 19:53:26 - INFO     | Saving data `dataset_1` to `data/dataset_1.pkl`\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\n",
    "DATA.dataset_1.save(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset_1.pkl',\n",
       " 'dataset_1.pkl.2023_03_04_19_45_16',\n",
       " 'dataset_1.pkl.2023_03_04_19_53_26']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('/code/examples/datasets_ipynb/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 19:53:30 - INFO     | Loading data `dataset_1` from `data/dataset_1.pkl`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  1  4\n",
       "1  2  5\n",
       "2  3  6"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA.dataset_1.load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset_1']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_f3ef8\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_f3ef8_level0_col0\" class=\"col_heading level0 col0\" >dataset</th>\n",
       "      <th id=\"T_f3ef8_level0_col1\" class=\"col_heading level0 col1\" >description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_f3ef8_row0_col0\" class=\"data row0 col0\" >dataset_1</td>\n",
       "      <td id=\"T_f3ef8_row0_col1\" class=\"data row0 col1\" >Dataset description</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0xffff579292d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(DATA.descriptions).style.hide(axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_83b4d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_83b4d_level0_col0\" class=\"col_heading level0 col0\" >dataset</th>\n",
       "      <th id=\"T_83b4d_level0_col1\" class=\"col_heading level0 col1\" >dependencies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_83b4d_row0_col0\" class=\"data row0 col0\" >dataset_1</td>\n",
       "      <td id=\"T_83b4d_row0_col1\" class=\"data row0 col1\" >['SNOWFLAKE.SCHEMA.TABLE']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0xffff579289d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(DATA.dependencies).style.hide(axis='index')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
