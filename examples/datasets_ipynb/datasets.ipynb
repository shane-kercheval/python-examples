{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "\n",
    "CURRENT_DIR = '/code/examples/datasets_ipynb'\n",
    "\n",
    "logging.config.fileConfig(\n",
    "    os.path.join(CURRENT_DIR, 'logging.conf'),\n",
    "    disable_existing_loggers=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Rectangle at 0xffff9d7ae1d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import abc\n",
    "\n",
    "class Shape(metaclass=abc.ABCMeta):\n",
    "    def __init__(self, a) -> None:\n",
    "        self.a = a\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def area(self):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def perimeter(self):\n",
    "        pass\n",
    "\n",
    "class Rectangle(Shape):\n",
    "    def __init__(self, length, width):\n",
    "        super().__init__(a=1)\n",
    "        self.length = length\n",
    "        self.width = width\n",
    "\n",
    "    def area(self):\n",
    "        return self.length * self.width\n",
    "\n",
    "    def perimeter(self):\n",
    "        return 2 * (self.length + self.width)\n",
    "\n",
    "Rectangle(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file defines classes that hides the logic/path for saving and loading specific datasets that\n",
    "are used across this project, as well as providing a brief description for each dataset.\n",
    "\n",
    "To define a new dataset, create a property in Datasets.__init__() following the existing patthern.\n",
    "\n",
    "The DATA variable is assigned an instance of the Datasets class and can be imported into other\n",
    "scripts/notebooks.\n",
    "\n",
    "To load the dataset called `the_dataset`, use the following code:\n",
    "\n",
    "```\n",
    "from source.services.data import DATA\n",
    "df = DATA.the_dataset.load()\n",
    "```\n",
    "\n",
    "To save the dataset called `the_dataset`, use the following code:\n",
    "\n",
    "```\n",
    "from source.services.data import DATA\n",
    "\n",
    "df = ...logic..\n",
    "DATA.the_dataset.save(df)\n",
    "```\n",
    "\"\"\"\n",
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "import pickle\n",
    "from abc import ABC, abstractmethod, abstractproperty\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class DataPersistence(ABC):\n",
    "    \"\"\"\n",
    "    Class that wraps the logic of saving/loading/describing a given dataset.\n",
    "    Meant to be subclassed with specific types of loaders (e.g. pickle, database, etc.)\n",
    "    \"\"\"\n",
    "    def __init__(self, description: str, dependencies: list):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            description: description of the dataset\n",
    "            dependencies: dependencies of the dataset\n",
    "        \"\"\"\n",
    "        self.description = description\n",
    "        self.dependencies = dependencies\n",
    "        self.name = None  # this is set dynamically\n",
    "\n",
    "    @abstractmethod\n",
    "    def _load(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _save(self, data):\n",
    "        pass\n",
    "\n",
    "    def load(self):\n",
    "        assert self.name\n",
    "        return self._load()\n",
    "    \n",
    "    def save(self, data):\n",
    "        assert self.name\n",
    "        self._save(data)\n",
    "\n",
    "class FileDataPersistence(DataPersistence):\n",
    "    \"\"\"\n",
    "    Class that wraps the logic of saving/loading/describing a given dataset to the file-system.\n",
    "    Adds logic for backing up datasets if they are being saved and already exist (i.e. renaming\n",
    "    the file with a timestamp)\n",
    "    Meant to be subclassed with specific types of loaders (e.g. pickle, csv, etc.)\n",
    "    \"\"\"\n",
    "    def __init__(self, description: str, dependencies: list, directory: str):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            description: description of the dataset\n",
    "            dependencies: dependencies of the dataset\n",
    "        \"\"\"\n",
    "        super().__init__(description, dependencies)\n",
    "        self.directory = directory\n",
    "\n",
    "    @abstractmethod\n",
    "    def _load(self):\n",
    "        \"\"\"Logic to load the `data`\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _save(self, data):\n",
    "        \"\"\"Logic to save the `data`\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractproperty\n",
    "    def path(self):\n",
    "        \"\"\"Full path (directory and file name) to load/save.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def load(self):\n",
    "        assert self.name\n",
    "        logging.info(f\"Loading data `{self.name}` from `{self.path}`\")\n",
    "        return self._load()\n",
    "    \n",
    "    def save(self, data):\n",
    "        assert self.name\n",
    "        logging.info(f\"Saving data `{self.name}` to `{self.path}`\")\n",
    "        # if the file already exists, save it to another name\n",
    "        if os.path.isfile(self.path):\n",
    "            timestamp = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "            new_name = self.path + '.' + timestamp\n",
    "            logging.info(f\"Backing up current data `{self.name}` to `{new_name}`\")\n",
    "            os.rename(self.path, new_name)\n",
    "        self._save(data)\n",
    "\n",
    "\n",
    "class PickledDataLoader(FileDataPersistence):\n",
    "    \"\"\"\n",
    "    Class that wraps the logic of saving/loading/describing a given dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, description: str, dependencies: list, directory: str):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            description: description of the dataset\n",
    "            dependencies: dependencies of the dataset\n",
    "            directory:\n",
    "                the directory to save to and load from. NOTE: this should **not** contain the file name\n",
    "                which is assigned at a later point in time based on the property name in the\n",
    "                `Datasets` class.\n",
    "        \"\"\"\n",
    "        super().__init__(description, dependencies, directory)\n",
    "\n",
    "    @property\n",
    "    def path(self):\n",
    "        return os.path.join(self.directory, self.name + '.pkl')\n",
    "\n",
    "    def _load(self):\n",
    "        with open(self.path, 'rb') as handle:\n",
    "            unpickled_object = pickle.load(handle)\n",
    "        return unpickled_object\n",
    "\n",
    "    def _save(self, data):\n",
    "        with open(self.path, 'wb') as handle:\n",
    "            pickle.dump(data, handle)\n",
    "\n",
    "\n",
    "class CsvDataLoader(FileDataPersistence):\n",
    "    \"\"\"\n",
    "    Class that wraps the logic of saving/loading/describing a given dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, description: str, dependencies: list, directory: str):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            description: description of the dataset\n",
    "            dependencies: dependencies of the dataset\n",
    "            directory:\n",
    "                the path to save to and load from. NOTE: this should **not** contain the file name\n",
    "                which is assigned at a later point in time based on the property name in the\n",
    "                `Datasets` class.\n",
    "        \"\"\"\n",
    "        super().__init__(description, dependencies, directory)\n",
    "\n",
    "    @property\n",
    "    def path(self):\n",
    "        return os.path.join(self.directory, self.name + '.pkl')\n",
    "\n",
    "    def _load(self):\n",
    "        return pd.read_csv(self.path)\n",
    "\n",
    "    def _save(self, data: pd.DataFrame):\n",
    "        data.to_csv(self.path, index=None)\n",
    "\n",
    "\n",
    "class Datasets:\n",
    "    \"\"\"class that defines all of the datasets available globally to the project.\"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Use this function to define datasets by following the existing pattern.\"\"\"\n",
    "\n",
    "        ####\n",
    "        # DEFINE DATASETS HERE\n",
    "        ####\n",
    "        self.dataset_1 = PickledDataLoader(\n",
    "            description=\"Dataset description\",\n",
    "            dependencies=['SNOWFLAKE.SCHEMA.TABLE'],\n",
    "            directory=os.path.join(CURRENT_DIR, 'data'),\n",
    "        )\n",
    "        self.other_dataset_2 = PickledDataLoader(\n",
    "            description=\"Other dataset description\",\n",
    "            dependencies=['dataset_1'],\n",
    "            directory=os.path.join(CURRENT_DIR, 'data'),\n",
    "        )\n",
    "        self.dataset_3_csv = CsvDataLoader(\n",
    "            description=\"Other dataset description\",\n",
    "            dependencies=['dataset_1'],\n",
    "            directory=os.path.join(CURRENT_DIR, 'data'),\n",
    "        )\n",
    "        ####\n",
    "        # END OF DATASETS\n",
    "        ####\n",
    "        # dynamically set the name property in the DataPersistence object in all of the object;\n",
    "        # I don't love this design, but it forces the names to match the property name and reduces\n",
    "        # the redundancy of duplicating the name when defining the property and passing in the name\n",
    "        # ot the loader\n",
    "        for dataset in self.datasets:\n",
    "            dataset_obj = getattr(self, dataset)\n",
    "            dataset_obj.name = dataset\n",
    "\n",
    "    @property\n",
    "    def datasets(self) -> list[str]:\n",
    "        \"\"\"Returns the names of the datasets available.\"\"\"\n",
    "        ignore = set(['datasets', 'descriptions', 'dependencies'])\n",
    "        return [\n",
    "            attr for attr in dir(self)\n",
    "            if attr not in ignore and isinstance(getattr(self, attr), DataPersistence)\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def descriptions(self) -> dict[str]:\n",
    "        \"\"\"Returns the names and descriptions of the datasets available.\"\"\"\n",
    "        return [\n",
    "            dict(\n",
    "                dataset=x,\n",
    "                description=getattr(self, x).description\n",
    "            )\n",
    "            for x in self.datasets\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def dependencies(self) -> dict[str]:\n",
    "        \"\"\"Returns the names and dependencies of the datasets available.\"\"\"\n",
    "        return [\n",
    "            dict(\n",
    "                dataset=x,\n",
    "                dependencies=getattr(self, x).dependencies\n",
    "            )\n",
    "            for x in self.datasets\n",
    "        ]\n",
    "\n",
    "\n",
    "# create a global object that can be imported into other scripts\n",
    "DATA = Datasets()\n",
    "\n",
    "# ensure all names got set properly\n",
    "assert all([getattr(DATA, x).name == x for x in DATA.datasets])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store', '.gitkeep']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(CURRENT_DIR, 'data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 20:51:02 - INFO     | Saving data `dataset_1` to `/code/examples/datasets_ipynb/data/dataset_1.pkl`\n",
      "2023-03-19 20:51:02 - INFO     | Saving data `dataset_3_csv` to `/code/examples/datasets_ipynb/data/dataset_3_csv.pkl`\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\n",
    "DATA.dataset_1.save(df)\n",
    "DATA.dataset_3_csv.save(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store', '.gitkeep', 'dataset_1.pkl', 'dataset_3_csv.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(CURRENT_DIR, 'data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 20:51:06 - INFO     | Loading data `dataset_1` from `/code/examples/datasets_ipynb/data/dataset_1.pkl`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  1  4\n",
       "1  2  5\n",
       "2  3  6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA.dataset_1.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 20:51:07 - INFO     | Loading data `dataset_3_csv` from `/code/examples/datasets_ipynb/data/dataset_3_csv.pkl`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  1  4\n",
       "1  2  5\n",
       "2  3  6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA.dataset_3_csv.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 20:51:07 - INFO     | Loading data `dataset_1` from `/code/examples/datasets_ipynb/data/dataset_1.pkl`\n",
      "2023-03-19 20:51:07 - INFO     | Loading data `dataset_3_csv` from `/code/examples/datasets_ipynb/data/dataset_3_csv.pkl`\n"
     ]
    }
   ],
   "source": [
    "assert (DATA.dataset_1.load() == DATA.dataset_3_csv.load()).all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 20:51:11 - INFO     | Saving data `dataset_1` to `/code/examples/datasets_ipynb/data/dataset_1.pkl`\n",
      "2023-03-19 20:51:11 - INFO     | Backing up current data `dataset_1` to `/code/examples/datasets_ipynb/data/dataset_1.pkl.2023_03_19_20_51_11`\n"
     ]
    }
   ],
   "source": [
    "df = df.replace(2, 'a')\n",
    "DATA.dataset_1.save(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 20:51:27 - INFO     | Saving data `dataset_3_csv` to `/code/examples/datasets_ipynb/data/dataset_3_csv.pkl`\n",
      "2023-03-19 20:51:27 - INFO     | Backing up current data `dataset_3_csv` to `/code/examples/datasets_ipynb/data/dataset_3_csv.pkl.2023_03_19_20_51_27`\n"
     ]
    }
   ],
   "source": [
    "df = df.replace(2, 'a')\n",
    "DATA.dataset_3_csv.save(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " '.gitkeep',\n",
       " 'dataset_1.pkl',\n",
       " 'dataset_1.pkl.2023_03_19_20_51_11',\n",
       " 'dataset_3_csv.pkl',\n",
       " 'dataset_3_csv.pkl.2023_03_19_20_51_27']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(CURRENT_DIR, 'data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 20:51:54 - INFO     | Loading data `dataset_1` from `/code/examples/datasets_ipynb/data/dataset_1.pkl`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  1  4\n",
       "1  a  5\n",
       "2  3  6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA.dataset_1.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 20:52:27 - INFO     | Loading data `dataset_3_csv` from `/code/examples/datasets_ipynb/data/dataset_3_csv.pkl`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  1  4\n",
       "1  a  5\n",
       "2  3  6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA.dataset_3_csv.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 20:53:01 - INFO     | Saving data `other_dataset_2` to `/code/examples/datasets_ipynb/data/other_dataset_2.pkl`\n"
     ]
    }
   ],
   "source": [
    "DATA.other_dataset_2.save(df.replace('a', 'asdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 20:53:02 - INFO     | Loading data `other_dataset_2` from `/code/examples/datasets_ipynb/data/other_dataset_2.pkl`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>asdf</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      a  b\n",
       "0     1  4\n",
       "1  asdf  5\n",
       "2     3  6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA.other_dataset_2.load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset_1', 'dataset_3_csv', 'other_dataset_2']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_c9fc1\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_c9fc1_level0_col0\" class=\"col_heading level0 col0\" >dataset</th>\n",
       "      <th id=\"T_c9fc1_level0_col1\" class=\"col_heading level0 col1\" >description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_c9fc1_row0_col0\" class=\"data row0 col0\" >dataset_1</td>\n",
       "      <td id=\"T_c9fc1_row0_col1\" class=\"data row0 col1\" >Dataset description</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c9fc1_row1_col0\" class=\"data row1 col0\" >dataset_3_csv</td>\n",
       "      <td id=\"T_c9fc1_row1_col1\" class=\"data row1 col1\" >Other dataset description</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c9fc1_row2_col0\" class=\"data row2 col0\" >other_dataset_2</td>\n",
       "      <td id=\"T_c9fc1_row2_col1\" class=\"data row2 col1\" >Other dataset description</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0xffff9c61ffa0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(DATA.descriptions).style.hide(axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_64574\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_64574_level0_col0\" class=\"col_heading level0 col0\" >dataset</th>\n",
       "      <th id=\"T_64574_level0_col1\" class=\"col_heading level0 col1\" >dependencies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_64574_row0_col0\" class=\"data row0 col0\" >dataset_1</td>\n",
       "      <td id=\"T_64574_row0_col1\" class=\"data row0 col1\" >['SNOWFLAKE.SCHEMA.TABLE']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_64574_row1_col0\" class=\"data row1 col0\" >dataset_3_csv</td>\n",
       "      <td id=\"T_64574_row1_col1\" class=\"data row1 col1\" >['dataset_1']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_64574_row2_col0\" class=\"data row2 col0\" >other_dataset_2</td>\n",
       "      <td id=\"T_64574_row2_col1\" class=\"data row2 col1\" >['dataset_1']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0xffff5d39ded0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(DATA.dependencies).style.hide(axis='index')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
