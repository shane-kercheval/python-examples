{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "\n",
    "CURRENT_DIR = '/code/examples/datasets_ipynb'\n",
    "\n",
    "logging.config.fileConfig(\n",
    "    os.path.join(CURRENT_DIR, 'logging.conf'),\n",
    "    disable_existing_loggers=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file defines classes that hides the logic/path for saving and loading specific datasets that\n",
    "are used across this project, as well as providing a brief description for each dataset.\n",
    "\n",
    "To define a new dataset, create a property in Datasets.__init__() following the existing patthern.\n",
    "\n",
    "The DATA variable is assigned an instance of the Datasets class and can be imported into other\n",
    "scripts/notebooks.\n",
    "\n",
    "To load the dataset called `the_dataset`, use the following code:\n",
    "\n",
    "```\n",
    "from source.services.data import DATA\n",
    "df = DATA.the_dataset.load()\n",
    "```\n",
    "\n",
    "To save the dataset called `the_dataset`, use the following code:\n",
    "\n",
    "```\n",
    "from source.services.data import DATA\n",
    "\n",
    "df = ...logic..\n",
    "DATA.the_dataset.save(df)\n",
    "```\n",
    "\"\"\"\n",
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "\n",
    "def read_pickle(path):\n",
    "    \"\"\"\n",
    "    Simple helper function that read's from a pickled object.\n",
    "\n",
    "    Args:\n",
    "        path:\n",
    "            File path where the pickled object will be stored.\n",
    "    \"\"\"\n",
    "    with open(path, 'rb') as handle:\n",
    "        unpickled_object = pickle.load(handle)\n",
    "    return unpickled_object\n",
    "\n",
    "\n",
    "def to_pickle(obj, path):\n",
    "    \"\"\"\n",
    "    Helper function that saves a pickled object.\n",
    "\n",
    "    NOTE: If there is an existing file that matches `path`, it will rename that file by appending\n",
    "    the current timestamp. The intent is to cache files in case there is an issue or any desire\n",
    "    to look at past datasets.\n",
    "\n",
    "    Args:\n",
    "        obj:\n",
    "            the object to save\n",
    "        path:\n",
    "            File path where the pickled object will be read from.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(path):\n",
    "        timestamp = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "        os.rename(path, path + '.' + timestamp)\n",
    "\n",
    "    with open(path, 'wb') as handle:\n",
    "        pickle.dump(obj, handle)\n",
    "\n",
    "\n",
    "class DataWrapper:\n",
    "    \"\"\"class that wraps the logic of saving/loading/describing a given dataset.\"\"\"\n",
    "    def __init__(self, description: str, dependencies: list, path: str):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            description: description of the dataset\n",
    "            dependencies: dependencies of the dataset\n",
    "            path:\n",
    "                the path to save to and load from. NOTE: this should **not** contain the file name\n",
    "                which is assigned at a later point in time based on the property name in the\n",
    "                `Datasets` class.\n",
    "        \"\"\"\n",
    "        self.description = description\n",
    "        self.dependencies = dependencies\n",
    "        self.path = path\n",
    "        self.name = None\n",
    "\n",
    "    def load(self):\n",
    "        assert self.name\n",
    "        _file_name = os.path.join(self.path, self.name + '.pkl')\n",
    "        logging.info(f\"Loading data `{self.name}` from `{_file_name}`\")\n",
    "        return read_pickle(path=_file_name)\n",
    "\n",
    "    def save(self, data):\n",
    "        assert self.name\n",
    "        _file_name = os.path.join(self.path, self.name + '.pkl')\n",
    "        logging.info(f\"Saving data `{self.name}` to `{_file_name}`\")\n",
    "        to_pickle(obj=data, path=_file_name)\n",
    "\n",
    "\n",
    "class Datasets:\n",
    "    \"\"\"class that defines all of the datasets available globally to the project.\"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Use this function to define datasets by following the existing pattern.\"\"\"\n",
    "\n",
    "        ####\n",
    "        # DEFINE DATASETS HERE\n",
    "        ####\n",
    "        self.dataset_1 = DataWrapper(\n",
    "            description=\"Dataset description\",\n",
    "            dependencies=['SNOWFLAKE.SCHEMA.TABLE'],\n",
    "            path=os.path.join(CURRENT_DIR, 'data'),\n",
    "        )\n",
    "        self.other_dataset_2 = DataWrapper(\n",
    "            description=\"Other dataset description\",\n",
    "            dependencies=['dataset_1'],\n",
    "            path=os.path.join(CURRENT_DIR, 'data'),\n",
    "        )\n",
    "        ####\n",
    "        # END OF DATASETS\n",
    "        ####\n",
    "\n",
    "        # dynamically set the name property in the DataWrapper object in all of the object;\n",
    "        # this forces the names to match the property name and reduces the redundancy of\n",
    "        # duplicating the name when defining the property and passing in the name ot the loader\n",
    "        for dataset in self.datasets:\n",
    "            dataset_obj = getattr(self, dataset)\n",
    "            dataset_obj.name = dataset\n",
    "\n",
    "    @property\n",
    "    def datasets(self) -> list[str]:\n",
    "        \"\"\"Returns the names of the datasets available.\"\"\"\n",
    "        ignore = set(['datasets', 'descriptions', 'dependencies'])\n",
    "        return [\n",
    "            attr for attr in dir(self)\n",
    "            if attr not in ignore and isinstance(getattr(self, attr), DataWrapper)\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def descriptions(self) -> dict[str]:\n",
    "        \"\"\"Returns the names and descriptions of the datasets available.\"\"\"\n",
    "        return [\n",
    "            dict(\n",
    "                dataset=x,\n",
    "                description=getattr(self, x).description\n",
    "            )\n",
    "            for x in self.datasets\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def dependencies(self) -> dict[str]:\n",
    "        \"\"\"Returns the names and dependencies of the datasets available.\"\"\"\n",
    "        return [\n",
    "            dict(\n",
    "                dataset=x,\n",
    "                dependencies=getattr(self, x).dependencies\n",
    "            )\n",
    "            for x in self.datasets\n",
    "        ]\n",
    "\n",
    "\n",
    "# create a global object that can be imported into other scripts\n",
    "DATA = Datasets()\n",
    "\n",
    "# ensure all names got set properly\n",
    "assert all([getattr(DATA, x).name == x for x in DATA.datasets])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.gitkeep']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(CURRENT_DIR, 'data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 20:42:42 - INFO     | Saving data `dataset_1` to `/code/examples/datasets_ipynb/data/dataset_1.pkl`\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\n",
    "DATA.dataset_1.save(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.gitkeep', 'dataset_1.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(CURRENT_DIR, 'data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 20:42:42 - INFO     | Loading data `dataset_1` from `/code/examples/datasets_ipynb/data/dataset_1.pkl`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  1  4\n",
       "1  2  5\n",
       "2  3  6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA.dataset_1.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 20:42:42 - INFO     | Saving data `dataset_1` to `/code/examples/datasets_ipynb/data/dataset_1.pkl`\n"
     ]
    }
   ],
   "source": [
    "df = df.replace(2, 'a')\n",
    "DATA.dataset_1.save(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.gitkeep', 'dataset_1.pkl', 'dataset_1.pkl.2023_03_04_20_42_42']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(CURRENT_DIR, 'data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 20:42:43 - INFO     | Loading data `dataset_1` from `/code/examples/datasets_ipynb/data/dataset_1.pkl`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  1  4\n",
       "1  a  5\n",
       "2  3  6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA.dataset_1.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 20:42:43 - INFO     | Saving data `other_dataset_2` to `/code/examples/datasets_ipynb/data/other_dataset_2.pkl`\n"
     ]
    }
   ],
   "source": [
    "DATA.other_dataset_2.save(df.replace('a', 'asdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 20:42:43 - INFO     | Loading data `other_dataset_2` from `/code/examples/datasets_ipynb/data/other_dataset_2.pkl`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>asdf</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      a  b\n",
       "0     1  4\n",
       "1  asdf  5\n",
       "2     3  6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA.other_dataset_2.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  1  4\n",
       "1  a  5\n",
       "2  3  6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset_1', 'other_dataset_2']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_7c05d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_7c05d_level0_col0\" class=\"col_heading level0 col0\" >dataset</th>\n",
       "      <th id=\"T_7c05d_level0_col1\" class=\"col_heading level0 col1\" >description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_7c05d_row0_col0\" class=\"data row0 col0\" >dataset_1</td>\n",
       "      <td id=\"T_7c05d_row0_col1\" class=\"data row0 col1\" >Dataset description</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7c05d_row1_col0\" class=\"data row1 col0\" >other_dataset_2</td>\n",
       "      <td id=\"T_7c05d_row1_col1\" class=\"data row1 col1\" >Other dataset description</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0xffff3f11e140>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(DATA.descriptions).style.hide(axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_6914a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_6914a_level0_col0\" class=\"col_heading level0 col0\" >dataset</th>\n",
       "      <th id=\"T_6914a_level0_col1\" class=\"col_heading level0 col1\" >dependencies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_6914a_row0_col0\" class=\"data row0 col0\" >dataset_1</td>\n",
       "      <td id=\"T_6914a_row0_col1\" class=\"data row0 col1\" >['SNOWFLAKE.SCHEMA.TABLE']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6914a_row1_col0\" class=\"data row1 col0\" >other_dataset_2</td>\n",
       "      <td id=\"T_6914a_row1_col1\" class=\"data row1 col1\" >['dataset_1']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0xffff3dcbf400>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(DATA.dependencies).style.hide(axis='index')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
